{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Residual networks\n",
    "\n",
    "## General instructions\n",
    "\n",
    "Each week you will be given an assignment related to the associated module. You have roughly one week to complete and submit each of them. There are 3 weekly group sessions available to help you complete the assignments. Attendance is not mandatory but recommended. However, assignments are graded each week and not submitting them or submitting them after the deadline will give you no points.\n",
    "\n",
    "**FORMAT**: Jupyter notebook **(single file, not in a zip please!)**\n",
    "\n",
    "**DEADLINE**: Sunday 7th March, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this assignment is to get a better intuition as to why residual networks work in practice. More particularly, we will investigate how adding skip connections to a feeforward neural network affects its gradient when combined with carefull initialization of the weights and biases and batch normalization. In all the present work, **we will solely investigate networks' initialization, that is the statistics of sampled neural networks without any training.** We will in particular emphasize how the depth of feedforward neural networks tends to \"break\" the gradients, which is known to make training much more difficult. \n",
    "\n",
    "Observing the gradients is made difficult by the fact that a neural network typically has many variables. To tackle this problem, rather than computing the gradients with respect to the network's parameters as you are now used to, we will instead compute the gradients with respect to the input data. This is meaningful since by the chain rule, the derivatives with respect to inputs are connected to the derivatives with respect to parameters. Furthermore, we will investigate a neural network $x \\mapsto f(x)$ mapping $\\mathbb{R}$ to $\\mathbb{R}$, making it convenient for us to investigate the one dimensional derivatives with respect to inputs. Since we are interested in observing how network depth negatively affects the structure of gradients, we will use a simple grid of uniformly spaced data points ranging from -1 to 1 as input for our network. The exact structure of the neural network is given in the image below:\n",
    "\n",
    "![title](a_simple_NN-1.png)\n",
    "\n",
    "\n",
    "Your task is to complete all the cells from the notebook (more detailed instructions are provided in each cell). **In the last cell, you will be asked to provide a thorough visualization (with plots !) and analysis of your experiments. Do not take this lightly as it will be the most important aspect for the grading.**\n",
    "\n",
    "\n",
    "## Andrew's Videos related to this week's assignment\n",
    "\n",
    "- [C4W2L03 ResNets](https://www.youtube.com/watch?v=ZILIbUvp5lk&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=14)\n",
    "- [C4W2L04 Why ResNets work](https://www.youtube.com/watch?v=RYth6EbBUqM&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=15)\n",
    "\n",
    "## Going further\n",
    "\n",
    "- https://arxiv.org/pdf/1702.08591.pdf\n",
    "- https://arxiv.org/pdf/1805.07477.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn  as sns\n",
    "import math\n",
    "import copy\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "(You will use these later !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(array):\n",
    "    correlation = np.corrcoef(array, rowvar=False)\n",
    "    autocorrelation = correlation[:,0]\n",
    "    return correlation, autocorrelation\n",
    "\n",
    "def visualize_white_noise(num_vars, num_samples, figsize, color):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    fig.suptitle(\"White noise: \" + \"$w_n \\\\sim \\\\mathcal{N}(0,\\\\sigma)\\\\quad \\\\forall n$\")\n",
    "    white_noise = np.random.randn(num_samples, num_vars)\n",
    "    correlation, autocorrelation = compute_correlation(white_noise)\n",
    "    axes[0].plot(np.arange(num_vars), white_noise.mean(axis=0), color=color)\n",
    "    axes[1].plot(np.arange(num_vars), autocorrelation, color=color)\n",
    "    sns.heatmap(correlation, cbar=True, xticklabels=False, yticklabels=False, ax=axes[2])\n",
    "    axes[0].set_title(\"Mean\")\n",
    "    axes[1].set_title(\"Autocorrelation function\")\n",
    "    axes[2].set_title(\"Correlation matrix\")\n",
    "    axes[0].set_xlabel(\"n\", fontsize=\"x-large\")\n",
    "    axes[1].set_xlabel(\"k\", fontsize=\"x-large\")\n",
    "    axes[0].set_ylabel(\"$E(w_n)$\", fontsize=\"x-large\")\n",
    "    axes[1].set_ylabel(\"$\\\\rho_w(0,0+k)$\", fontsize=\"x-large\")\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_gradients(gradients, mean, figsize, color, name, depth):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    fig.suptitle(name + \"gradients \" + \" depth \" + str(depth))\n",
    "    correlation, autocorrelation = compute_correlation(gradients)\n",
    "    axes[0].plot(np.arange(gradients.shape[0]), mean, color=color)\n",
    "    axes[1].plot(np.arange(gradients.shape[1]), autocorrelation, color=color)\n",
    "    sns.heatmap(correlation, cbar=True, xticklabels=False, yticklabels=False, ax=axes[2])\n",
    "    axes[0].set_title(\"Mean\")\n",
    "    axes[1].set_title(\"Autocorrelation function\")\n",
    "    axes[2].set_title(\"Correlation matrix\")\n",
    "    axes[0].set_xlabel(\"n\", fontsize=\"x-large\")\n",
    "    axes[1].set_xlabel(\"k\", fontsize=\"x-large\")\n",
    "    axes[0].set_ylabel(\"$E(w_n)$\", fontsize=\"x-large\")\n",
    "    axes[1].set_ylabel(\"$\\\\rho_w(0,0+k)$\", fontsize=\"x-large\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_wrt_input(neural_network, x):\n",
    "    '''\n",
    "    \n",
    "    Input: \n",
    "        -neural_network: a neural network f\n",
    "        -x: input data x=[x1,...,xk] (1D tensor)\n",
    "        \n",
    "    Output:\n",
    "        -gradient: the gradient of the neural network wrt x, df/dx=[df/dx1,...,df/dxk]\n",
    "        \n",
    "    TODO: \n",
    "    1) Disable autograd for all the network's parameters.\n",
    "    2) Enable autograd for x.\n",
    "    3) Implement a forward pass followed by a backward pass on the neural network with input data x.\n",
    "    4) Collect the gradient of the neural network in x.\n",
    "    \n",
    "    Note: We do not want to update the parameters of the neural network, thus we don't need an optimizer.\n",
    "    \n",
    "    '''\n",
    "    for param in neural_network.parameters():\n",
    "        param.requires_grad = False\n",
    "    x.requires_grad = True\n",
    "    out = neural_network(x).sum()\n",
    "    out.backward()\n",
    "    gradient = x.grad.view(1, -1)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual1D(nn.Module):\n",
    "    '''\n",
    "    \n",
    "    TODO:\n",
    "    1) Construct a feedforward neural network consisting of d layers such that:\n",
    "        -Each layer is exactly \"linear->batch_norm1d->ReLU\".\n",
    "        -Layer 0 has in_features=1, out_features=n.\n",
    "        -Layers 1 to d-2 have in_features=n, out_features=n.\n",
    "        -Layer d-1 has in_features=n, out_features=1.\n",
    "        -Each layer has its linear component initialized such that weight, bias ~ N(0,sigmaÂ²=1/in_features) \n",
    "    2) Implement the forward function of the neural network: a_{l+1} = layer_l(a_l), (a_0 = x).\n",
    "    3) Implement an alternative forward pass with skip connections: a_{l+1} = a_l + beta * layer_l(a_l), (a_0 = x).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d, n, beta, skip=True):\n",
    "        super(residual1D, self).__init__()\n",
    "        self.num_layers = d\n",
    "        self.num_layer_neurons = n\n",
    "        self.beta = beta\n",
    "        self.skip = skip\n",
    "        \n",
    "        self.initLinear = torch.nn.Linear(1, n)\n",
    "        self.linear = torch.nn.Linear(n, n)\n",
    "        self.lastLinear = torch.nn.Linear(n, 1)\n",
    "        \n",
    "        self.batch_norm1d = torch.nn.BatchNorm1d(n)\n",
    "        self.last_batch_norm1d = torch.nn.BatchNorm1d(1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "                    \n",
    "            \n",
    "            \n",
    "        self.initLinear.weight.data = torch.normal(0.0, 1.0, size=self.initLinear.weight.shape)\n",
    "        self.initLinear.bias.data = torch.normal(0.0, 1.0, size=self.initLinear.bias.shape)\n",
    "        \n",
    "        self.linear.weight.data = torch.normal(0.0, math.sqrt(1/n), size=self.linear.weight.shape)\n",
    "        self.linear.bias.data = torch.normal(0.0, math.sqrt(1/n), size=self.linear.bias.shape)\n",
    "        \n",
    "        self.lastLinear.weight.data = torch.normal(0.0, math.sqrt(1/n), size=self.lastLinear.weight.shape)\n",
    "        self.lastLinear.bias.data = torch.normal(0.0, math.sqrt(1/n), size=self.lastLinear.bias.shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Init\n",
    "        z_1 = self.initLinear(x)\n",
    "        z_1 = self.batch_norm1d(z_1)\n",
    "        a_n = self.relu(z_1)\n",
    "        \n",
    "        for layer in range(self.num_layers - 2):\n",
    "            z_n = self.linear(a_n)\n",
    "            z_n = self.batch_norm1d(z_n)\n",
    "            a_n = self.beta * self.relu(z_n) + a_n if self.skip else self.relu(z_n)\n",
    "        \n",
    "        z_last = self.lastLinear(a_n)\n",
    "        z_last = self.last_batch_norm1d(z_last)\n",
    "        out = self.beta * self.relu(z_last) + a_n if self.skip else self.relu(z_last)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_gradients(network_type, x, depths, num_iters, seed):\n",
    "    '''\n",
    "    \n",
    "    Input:\n",
    "        -network_type: type of neural network (either \"feedforward\" or \"residual\")\n",
    "        -x: input data x=[x1,...,xk] (1D tensor)\n",
    "        -depths: network depths to inspect\n",
    "        -num_iters: number of iterations (number of random sampling of neural networks)\n",
    "        -seed: self-explanatory\n",
    "    \n",
    "    TODO: \n",
    "    1) Seed pytorch\n",
    "    2) For every depth, for every iteration:\n",
    "        -Initialize a neural network of the desired type (\"feedforward\" or \"residual\") and depth.\n",
    "        -Compute and store the gradient of the sampled network wrt x.\n",
    "        -Stack the computed gradient vertically inside an array of shape [num_iters, X.shape[0]].\n",
    "    3) For every depth:\n",
    "        -Extract the mean of the gradients (with respect to the iterations).\n",
    "        -Extract the correlation matrix and the autocorrelation function of the gradients.\n",
    "    4) Visualize the statistics extracted in 3).\n",
    "    \n",
    "    Note: For 3) and 4), see Utility functions.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Bruk util funcs retard\n",
    "    torch.manual_seed(seed)\n",
    "    for depth in range(1, depths):\n",
    "        grads = torch.zeros(num_iters, x.shape[0])\n",
    "        means = []\n",
    "        for iteration in range(num_iters):\n",
    "            x_copy = copy.deepcopy(x)\n",
    "            model = residual1D(depths, x.shape[0], 0.25, skip=True) if network_type == \"residual\" else residual1D(depths, x.shape[0], 0.25, skip=False)\n",
    "            gradient = compute_gradient_wrt_input(model, x_copy)\n",
    "            grads[iteration] = gradient\n",
    "            means.append(torch.mean(gradient))\n",
    "        \n",
    "        visualize_gradients(grads, means, [20, 10], \"red\", network_type, depth)\n",
    "        visualize_white_noise(num_iters, x.shape[0], [20, 10], \"blue\")    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "TODO:\n",
    "1) Visualize white noise.\n",
    "2) Create a 1D data tensor x consisting of (>= 100) points uniformly spaced between [-1,1]\n",
    "3) Visualize gradients wrt x of feedforward neural networks with various depths.\n",
    "4) Visualize gradients wrt x of residual neural networks with various depths.\n",
    "5) ANALYSIS OF YOUR RESULTS (in a markdown cell): \n",
    "    -Give two problems occuring with the gradients of deep feedforward neural networks at initialization.\n",
    "    -From the observation of your plots, to which extent these problems were solved by \n",
    "     the combination of proper initialization + batch normalization + skip connections ?\n",
    "    -What is the maximal depth you can reach with your feedforward network before \"breaking\" the gradients ?\n",
    "    -Same question with your residual network.\n",
    "    -What is the effect of the parameter beta in your residual neural network ?\n",
    "    \n",
    "'''\n",
    "x = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "\n",
    "inspect_gradients(\"feedforward\", x, 15, 50, 27)\n",
    "inspect_gradients(\"residual\", x, 15, 50, 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Gradients may vanish or explode in deep feedforward neural networks\n",
    "\n",
    "Around 8 layers, the feedforward network starts resembling white noise.\n",
    "\n",
    "The residual layer continues to perform at much higher depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
