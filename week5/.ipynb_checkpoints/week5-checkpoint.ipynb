{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "Downloaded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "Write a code that loads this dataset by storing images (represented as np.arrays) and labels in variables X and y respectively, then splits X and y into train/validation/test sets and finally saves the images in .jpeg or .png format (we no longer need the labels) following this folders hierarchy: (image missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "df_values = pd.read_csv('handwritten_digits_images.csv')\n",
    "df_labels = pd.read_csv('handwritten_digits_labels.csv')\n",
    "\n",
    "X = df_values.to_numpy()\n",
    "y = df_labels.to_numpy()\n",
    "\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=27)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.15, shuffle=True, random_state=27)\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    im = Image.fromarray((x_train[i] * 255).astype(np.uint8).reshape(28, 28))\n",
    "    im.save(\"MNIST/train/\" + str(y_train[i][0]) + \"/\" + str(i) + \"-\" + str(y_train[i]) + \".jpg\")\n",
    "\n",
    "for i in range(len(x_val)):\n",
    "    im = Image.fromarray((x_val[i] * 255).astype(np.uint8).reshape(28, 28))\n",
    "    im.save(\"MNIST/val/\" + str(y_val[i][0]) + \"/\" + str(i) + \"-\" + str(y_val[i]) + \".jpg\")\n",
    "    \n",
    "for i in range(len(x_test)):\n",
    "    im = Image.fromarray((x_test[i] * 255).astype(np.uint8).reshape(28, 28))\n",
    "    im.save(\"MNIST/test/\" + str(y_test[i][0]) + \"/\" + str(i) + \"-\" + str(y_test[i]) + \".jpg\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Write a data preprocessor using the classes available in ’torchvision.transforms’\n",
    "that first converts images to grayscale, then pads two black pixels around\n",
    "images (output dimensions should become 32×32) and finally converts images to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocessor = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Pad(2),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "Use the ’torchvision.datasets.ImageFolder()’ class to create the MNIST train/validation/test datasets, given the paths obtained from the folders hierarchy in Question 2 and the data preprocessor created in Question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_set = datasets.ImageFolder(\"MNIST/train/\", transform=preprocessor)\n",
    "val_set = datasets.ImageFolder(\"MNIST/val/\", transform=preprocessor)\n",
    "test_set = datasets.ImageFolder(\"MNIST/test/\", transform=preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "Use the ’torch.utils.data.DataLoader()’ class to instantiate generators for the MNIST datasets generated in Question 4. Make sure that the train generator is set to shuffle data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_load = DataLoader(dataset=train_set, shuffle=True, batch_size=8, num_workers=2, drop_last=True)\n",
    "val_load = DataLoader(dataset=val_set, shuffle=True, batch_size=8, num_workers=2, drop_last=True)\n",
    "test_load = DataLoader(dataset=test_set, shuffle=True, batch_size=8, num_workers=2, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "In which context do we need data generators ? How do data generators typically work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7\n",
    "Write a simple MLP using the classes available in ’torch.nn’. Hint: You can use the class ’torch.nn.Sequential()’ to stack successive layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(1024, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8\n",
    "\n",
    "I read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9\n",
    "Adapt the codes from the aforementioned links to implement your custom training loop. Each iteration of this loop should process your whole training set once (this is called an epoch). During each epoch, you will perform forward and backward passes with the MLP created in Question 7 on the batches returned by your train generator. Every processed batch will be followed by an update of the neural network’s parameters. You can for instance use the categorical cross-entropy loss implemented in the class ’pytorch.nn.CrossEntropyLoss()’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    for x_batch, y_batch in train_load:\n",
    "        out = model(x_batch)\n",
    "        l = loss(out, y_batch)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10\n",
    "Expand your training loop to collect the training history: you will store the train accuracy and loss at each epoch. To obtain those, compute the running average of these metrics over the train batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.9901), tensor(0.9922), tensor(0.9916), tensor(0.9936), tensor(0.9927), tensor(0.9950), tensor(0.9948), tensor(0.9928), tensor(0.9938), tensor(0.9940)]\n"
     ]
    }
   ],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "for i in range(10):\n",
    "    n_accuracy = 0\n",
    "    n_loss = 0\n",
    "    for x_batch, y_batch in train_load:\n",
    "        out = model(x_batch)\n",
    "        loss = loss_func(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        n_accuracy += accuracy(out, y_batch)\n",
    "        n_loss += loss\n",
    "    accuracies.append(n_accuracy / len(train_load))\n",
    "    losses.append(n_loss / len(train_load))\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11\n",
    "Expand your training loop to perform a validation step (a forward pass on the validation dataset) and store the validation accuracy and loss at the end of every epoch. Hint: Make sure that your model has the right mode set (refer to the methods ’model.train()’ and ’model.eval()’). Also remember to disable pytorch’s autograd outside training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.9059)]\n",
      "[tensor(0.8265)]\n"
     ]
    }
   ],
   "source": [
    "val_accuracies = []\n",
    "val_losses = []\n",
    "for i in range(1): #TODO set epoques to 10\n",
    "    \n",
    "    #model.train()\n",
    "    #for x_batch, y_batch in train_load:\n",
    "    #    out = model(x_batch)\n",
    "    #    loss = loss_func(out, y_batch)\n",
    "    #    loss.backward()\n",
    "    #    optimizer.step()\n",
    "    #    optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \"\"\"losses, nums = zip(\n",
    "            *[loss_batch(model, loss_func, x_batch, y_batch) for x_batch, y_batch in val_load]\n",
    "        )\n",
    "    val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "    print(val_loss)\n",
    "        \"\"\"\n",
    "        val_accuracy_accumulator = 0\n",
    "        val_loss_accumulator = 0\n",
    "        for x_batch, y_batch in val_load:\n",
    "            out = model(x_batch)\n",
    "            loss = loss_func(out, y_batch)\n",
    "            val_accuracy_accumulator += accuracy(out, y_batch)\n",
    "            val_loss_accumulator += loss\n",
    "        val_accuracies.append((val_accuracy_accumulator / len(val_load)*100)\n",
    "        val_losses.append(val_loss_accumulator / len(val_load))\n",
    "\n",
    "print(val_accuracies)\n",
    "print(val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12\n",
    "Expand your training loop to perform an evaluation step (a forward pass on the test dataset) and report the test accuracy and loss at the end of the last epoch. Same hint as in Question 11 applies here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_accuracies = []\n",
    "with torch.no_grad():\n",
    "    n_accuracy = 0\n",
    "    for x_batch, y_batch in test_load:\n",
    "        out = model(x_batch)\n",
    "        n_accuracy += accuracy(out, y_batch)\n",
    "    test_accuracies.append(n_accuracy / len(test_load))\n",
    "print(test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13\n",
    "Find a configuration of hyperparameters such that your model reaches at least 90% accuracy on the test set. Indicate in your report the specifics of your architecture and the chosen hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14\n",
    "Produce plots for the train/validation accuracy and loss curves. Make sure to properly title your figures, legend your plots and label your axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
