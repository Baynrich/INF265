{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder, Variational AutoEncoder and GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Sequence\n",
    "import datetime\n",
    "import copy\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General instructions\n",
    "\n",
    "Every two weeks you will be given an assignment related to the associated module. There are 3 weekly group sessions available to help you complete the assignments, you are invited to attend one of them each week. Attendance is not mandatory but recommended. However, assignments are graded and not submitting them or submitting them after the deadline will give you no points. The grading system is detailed [here](https://mitt.uib.no/courses/27468/pages/general-information)\n",
    "\n",
    "**FORMAT**: Jupyter notebook    \n",
    "**DEADLINE**: Sunday 16th May, 23:59\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment we will go through 3 types of unsupervised neural network: AutoEncoder (AE), Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN). In the first section we will also introduce a new type of layer: the transpose convolution as it is widely used in these unsupervised methods.\n",
    "\n",
    "Unsupervised have many advantages including the fact that they don't need labels but they are also harder to train... In this assignment it will be totally okay if you don't get good results, we will provide examples of expected results and we don't expect you to spend the entire 2 weeks on the parameter tuning.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Transpose convolution\n",
    "2. AutoEncoder\n",
    "3. Variational AutoEncoder\n",
    "4. GAN\n",
    "\n",
    "## Related videos from the curriculum\n",
    "\n",
    "- [Lecture 15.1 — From PCA to autoencoders](https://www.youtube.com/watch?v=PSOt7u8u23w&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=69)\n",
    "- [Lecture 15.2 — Deep autoencoders](https://www.youtube.com/watch?v=6jhhIPdgkp0&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=70) \n",
    "- [Lecture 15.3 — Deep autoencoders for document retrieval](https://www.youtube.com/watch?v=ZCNbjpcX0yg&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=71)\n",
    "- [Lecture 15.6 — Shallow autoencoders for pre training](https://www.youtube.com/watch?v=xjlvVfEbhz4&list=PLLssT5z_DsK_gyrQ_biidwvPYCRNGI3iv&index=74)\n",
    "- [Lecture 13 | Generative Models]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. (Reminders) Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules \n",
    "\n",
    "In the cell below are defined the following modules that we will need in this section\n",
    "\n",
    "1. **MyEncoder**\n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "1. **MyDecoder**\n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module: \n",
    "    - input: image\n",
    "    - output: tensor `z` in latent space (lower dimension than input space)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=6, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=5, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=5, out_channels=4, kernel_size=4, stride=1)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=z_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        out = torch.relu(self.conv1(x))\n",
    "        out = torch.relu(self.conv2(out))\n",
    "        out = torch.relu(self.conv3(out))\n",
    "        out = out.view(N, -1)\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module: \n",
    "    - input: tensor `z` in latent space (lower dimension than image space)\n",
    "    - output: reconstructed image\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        c1 = 3\n",
    "        self.fc1 = nn.Linear(z_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 18*18)\n",
    "        self.transconv3 = nn.ConvTranspose2d(in_channels=1, out_channels=1,  kernel_size=3, stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, z_dim = x.shape\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = out.view(N, 1, 18, 18)\n",
    "        out = torch.sigmoid(self.transconv3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n",
    "\n",
    "Some useful functions:\n",
    "\n",
    "- **load_MNIST**: Return MNIST train and val dataset\n",
    "- **plot_true_VS_reconstructed**: Plot side by side original images with their reconstructed counterparts using a trained VAE\n",
    "- **plot_generated_images**: Plot images generated by a VAE\n",
    "- **training_vae**: Training loop for a VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = transforms.Compose([\n",
    "    transforms.CenterCrop(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.2475, 0.3892),\n",
    "]) \n",
    "\n",
    "\n",
    "def load_MNIST(data_path='../data/', transform = preprocessor, labels_kept=[0,1,3,4,8]):\n",
    "    \"\"\"\n",
    "    Return MNIST train and val dataset\n",
    "    \"\"\"\n",
    "    MNIST_train = datasets.MNIST(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,   \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    MNIST_val = datasets.MNIST(\n",
    "        data_path, \n",
    "        train=False,      \n",
    "        download=True,   \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    print('Size of the original training dataset: ', len(MNIST_train))\n",
    "    print('Size of the original validation dataset: ', len(MNIST_val))\n",
    "\n",
    "    if len(labels_kept) <10:\n",
    "        MNIST_train_reduced = [(img, labels_kept.index(label)) for img, label in MNIST_train if label in labels_kept]\n",
    "        MNIST_val_reduced = [(img, labels_kept.index(label)) for img, label in MNIST_val if label in labels_kept]\n",
    "\n",
    "        print('Size of the reduced training dataset: ', len(MNIST_train_reduced))\n",
    "        print('Size of the reduced validation dataset: ', len(MNIST_val_reduced))\n",
    "    else:\n",
    "        MNIST_train_reduced = MNIST_train\n",
    "        MNIST_val_reduced = MNIST_val\n",
    "\n",
    "    return MNIST_train_reduced, MNIST_val_reduced\n",
    "\n",
    "def plot_true_VS_reconstructed(ae, imgs):\n",
    "    \"\"\"\n",
    "    Plot side by side original images with their reconstructed counterpart using a trained VAE\n",
    "    \"\"\"\n",
    "    ae.eval()\n",
    "    N_img = 25\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=10, figsize=(10,6), sharex=True, sharey=True)\n",
    "    for i, img in enumerate(imgs[:N_img]):\n",
    "        with torch.no_grad():\n",
    "            out = ae(img.unsqueeze(0))\n",
    "            # True image\n",
    "            axs.flat[2*i].imshow(img.permute(1, 2, 0), cmap='Greys')\n",
    "            # Reconstruction\n",
    "            axs.flat[2*i + 1].imshow(out.squeeze(0).permute(1, 2, 0), cmap='Greys') \n",
    "            # Set ax title for the first row\n",
    "            if i<5:\n",
    "                axs.flat[2*i].set_title(\"True\\nimage\")\n",
    "                axs.flat[2*i + 1].set_title(\"AE recon-\\nstruction\")\n",
    "    return fig, axs\n",
    "\n",
    "def plot_generated_images(vae):\n",
    "    \"\"\"\n",
    "    Plot images generated by a VAE\n",
    "    \"\"\"\n",
    "    vae.eval()\n",
    "    N_img = 100\n",
    "    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(13,13), sharex=True, sharey=True, tight_layout=True)\n",
    "    fig.suptitle(\"Image generation\", fontsize=15)\n",
    "    for i in range(N_img):\n",
    "        with torch.no_grad():\n",
    "            a_z = torch.randn(1,vae.z_dim)\n",
    "            a_img = vae.decoder(a_z)\n",
    "            axs.flat[i].imshow(a_img[0].permute(1, 2, 0), cmap='Greys')\n",
    "    return fig, axs\n",
    "\n",
    "def training_vae(n_epochs, optimizer, model, loss_fn, train_loader, kld_weight=None, device=None):\n",
    "    \"\"\"\n",
    "    Training loop for a VAE\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "    if kld_weight is None:\n",
    "        kld_weight = torch.ones(n_epochs)\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        loss_train_mse = 0.\n",
    "        loss_train_kld = 0.\n",
    "\n",
    "        for imgs in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device) \n",
    "\n",
    "            outputs = model(imgs)\n",
    "            mse_loss, kld_loss = loss_fn(outputs, imgs, model.mu, model.logvar)\n",
    "            # Final loss is the sum of the 2 terms (with potentially a weight term)\n",
    "            loss = mse_loss + kld_weight[epoch-1]*kld_loss\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            loss_train_mse += mse_loss.item()\n",
    "            loss_train_kld += kld_loss.item()\n",
    "\n",
    "        \n",
    "        if epoch == 1 or epoch % 1 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}  |  MSE loss {:.3f}  |  KLD loss {:.12f}'.format(\n",
    "                datetime.datetime.now(), \n",
    "                epoch,\n",
    "                loss_train / len(train_loader),\n",
    "                loss_train_mse / len(train_loader),\n",
    "                loss_train_kld / len(train_loader),\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# It's already hard enough to train VAE, a subset of MNIST will be more than enough.\n",
    "labels_kept = [0,1,3,4,8]\n",
    "data_train, data_val = load_MNIST(labels_kept=labels_kept)\n",
    "imgs_train = [img for img, _ in data_train]\n",
    "imgs_val = [img for img, _ in data_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Variational AutoEncoder\n",
    "\n",
    "*related videos from the curriculum*\n",
    "\n",
    "- [Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=17) \n",
    "  - from 27:05 to 31:05: Introducting VAE \n",
    "  - *Let's forget about tractability :) *\n",
    "  - from 40:55 to 44:00: VAE loss and VAE training\n",
    "  - from 44:00 to 49:00: Generating data using VAE and summary \n",
    "\n",
    "**Introduction to VAE**\n",
    "\n",
    "A Variational AutoEncoder (VAE) is a neural network that is similar to a AE in its structure as it is composed of 2 sub-networks: an Encoder and a Decoder. However now our main objective is not to efficiently represent some data lying on a non-linear manifold anymore. Instead we aim at generating some new data that would look like the training data but that is just a simple copy of a training input! We want **new** data.\n",
    "\n",
    "To do so, we want our model to first get a good representation of what real data look like and then to be able to generate new plausible instances. The *get a good representation* part seems to be similar to what an Encoder can do and the *generate plausible instances* part to what a Decoder can do. However we can not really *generate new* data with a Decoder but just *reconstruct*. The *generate* part is actually what makes VAE different from AE and it is achieved in 2 steps: \n",
    "\n",
    "- a reparameterization step in the forward pass between the Encoder and Decoder\n",
    "- a KL-divergence term added to the loss function \n",
    "\n",
    "These 2 steps aim at forcing the elements of latent space to look like normally distributed samples. Once the training is finished, this forcing will allow us to generate new data by simply giving a random normally distributed sample to the Decoder (and the Encoder can be thrown away, so this is the opposite of AE where we could throw away the Decoder and keep the Encoder).\n",
    "\n",
    "**Reparameterization**\n",
    "\n",
    "The reparameterization consists in defining the latent vector ``z`` not as the output of the encoder but as a random sample from $\\mathcal{N}$(``mu``, ``std``$)$ where ``mu`` and ``std`` are the actual outputs of the Encoder (for computational reasons the Encoder actually returns ``mu`` and ``logvar (=log(std**2))``)\n",
    "\n",
    "**KL-divergence**\n",
    "\n",
    "The second difference between a VAE and an AE is in the loss function. In addition to the reconstruction term we want to force the Encoder to learn the parameters of a normal distribution. There exists a measure for that, the Kullback–Leibler divergence or simply KL-divergence ([Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kld#torch.nn.KLDivLoss)). The KL-divergence measures how one probability distribution $P$ is different from a second $Q$ by computing the following:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\int_{-\\infty}^{+\\infty} p(x)log\\Big(\\frac{p(x)}{q(x)}\\Big) \\,dx \\$$\n",
    "\n",
    "Where $p$ and $q$ are the probability densities of the probability distributions $P$ and $Q$. This measure can be used for different purposes and have as many interpretations (see [Interpretations](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Interpretations) section from the Wikipedia page) but in the context of Variational AutoEncoder we will interpret $Q$ as our prior and $P$ as our *true* distribution and $D_{KL}(P||Q)$ can then be interpreted as the information lost when our prior $Q$ is used to approximate $P$. In our very specific case where $Q \\sim \\mathcal{N}(0, 1)$ and $P \\sim \\mathcal{N}$(``mu``, ``std``$)$, $D_{KL}(P||Q)$ can be formulated as follows (see [Multivariate normal distributions](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions) section from the Wikipedia page):\n",
    "\n",
    "$$D_{KL}(P||Q) = \\frac{1}{2} \\sum_{i=1}^k \\Big( \\sigma_i^2 + \\mu_i^2 -1 -log(\\sigma_i^2) \\Big) \\qquad \\text{with std} = \\begin{bmatrix}\\sigma_0 \\cdots \\sigma_{z_{dim}-1} \\end{bmatrix} \\quad \\text{and mu} = \\begin{bmatrix}\\mu_0 \\cdots \\mu_{z_{dim}-1} \\end{bmatrix}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## TODO\n",
    "\n",
    "### MyVAE class: A variational AutoEncoder\n",
    "\n",
    "Complete the ``MyVAE`` class below (that is the variational counterpart of the ``MyAE`` class implemented in section 2). Normally you don't have to re-implement an encoder and a decoder, you can simply use the ``MyEncoder`` and the ``MyDecoder`` classes.  (defined at the beginning of this notebook as well). However a few details must be adapted:\n",
    "\n",
    "1. Encoder must return 2 tensors of shape ``(N, z_dim)``: one for ``mu`` and one for ``logvar (=log(std**2))`` **or equivalently** use ``MyEncoder`` with ``z_dim = 2*z_dim`` and then define ``mu`` and ``logvar`` in the forward method as follows: \n",
    "\n",
    "  ```\n",
    "  self.mu_logvar = self.encoder(x)               # Output of the encoder, shape=(N, 2*z_dim)\n",
    "  self.mu = self.mu_logvar[:,:self.z_dim]        # mu                   , shape=(N, z_dim)   (1st half of the encoded vector)\n",
    "  self.logvar = self.mu_logvar[:,self.z_dim:]    # logvar               , shape=(N, z_dim)   (2nd half of the encoded vector)\n",
    "  ```\n",
    "\n",
    "2. On top of returning the reconstructed image, the forward pass of your VAE must store ``mu`` and ``log`` (as suggested in the lines above) because we'll need them when computing the KL-divergence term of the loss function.\n",
    "\n",
    "3. A ``reparameterization`` method must be added that draws a sample $z$ (of shape ``(N, z_dim)``) from $\\mathcal{N}(0,1)$ and return $z*std + mu$ so that it corresponds to a sample from $\\mathcal{N}$(``mu``, ``std``$)$ (Reminder: ``logvar = log(std**2))`` so ``std = 1/2 * exp(logvar)``). **Hint** you can use [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html?highlight=randn#torch.randn) or [torch.randn_like](https://pytorch.org/docs/stable/generated/torch.randn_like.html?highlight=randn#torch.randn_like) or [torch.nn.init.normal_](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_). This method is to be called between the Encoder and the Decoder both during the training and can also be thrown away once the training is complete.\n",
    "\n",
    "4. Decoder is exactly same as for an AutoEncoder\n",
    "\n",
    "5. Write a ``generate_images`` method that takes as parameter an integer ``N_imgs`` defining the number of images to generate and returns ``imgs_generated`` (shape ``(N_imgs, C_in, H_in, W_in)``) the images generated by the VAE. **Hint** you can use [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html?highlight=randn#torch.randn)\n",
    "\n",
    "### loss_VAE: A loss adapted to VAE\n",
    "\n",
    "Complete the ``loss_VAE`` function below where \n",
    "- ``inputs`` is the original images (shape ``(N, C_in, H_in, W_in)``)\n",
    "- ``outputs`` is the reconstructed images (shape ``(N, C_in, H_in, W_in)``), \n",
    "- ``mu`` and ``logvar (=log(std**2)`` are the outputs of the Encoder representing the parameters of our normal distribution $P$  (both of shape ``(N, z_dim)``)). \n",
    "\n",
    "It returns the 2 terms of the VAE loss function:\n",
    "- ``mse_loss``, the reconstruction term. **Hint** you can use [F.mse_loss](https://pytorch.org/docs/stable/nn.functional.html?highlight=mse_loss#torch.nn.functional.mse_loss) with ``reduction=\"mean\"``\n",
    "- ``kld_loss``, the KL-divergence term. which is defined in the cell above. To adapt formula to batch computations, we need to re-write it as follows:\n",
    "\n",
    "$$D_{KL}(P||Q) =\\text{mean}\\Big( \\frac{1}{2} \\sum_{i=1}^k \\Big( \\sigma_{:,i}^2 + \\mu_{:,i}^2 -1 -log(\\sigma_{:,i}^2) \\Big) \\Big) \\qquad \\text{with std} = \\begin{bmatrix}\\sigma_{:,0} \\cdots \\sigma_{:, z_{dim}-1} \\end{bmatrix} \\quad \\text{and mu} = \\begin{bmatrix}\\mu_{:,0} \\cdots \\mu_{:, z_{dim}-1} \\end{bmatrix}  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__() \n",
    "        # Latent space dimension\n",
    "        self.z_dim = z_dim\n",
    "        # Encoder similar to what we used for the AE but used to encode both mu and logvar \n",
    "        self.encoder = #TODO\n",
    "        # There is no difference between a VAE and AE decoder\n",
    "        self.decoder = #TODO\n",
    "\n",
    "    def reparameterize(self):\n",
    "        \"\"\"\n",
    "        Reparameterization: draw a sample z of shape (N, z_dim) (z~N(mu, logvar))\n",
    "\n",
    "        mu and logvar should be accessible via self.mu and self.logvar\n",
    "        \"\"\"\n",
    "        # Initialize a vector z with the right shape whose elements are drawn from a normal distribution N(0, 1)\n",
    "        #TODO\n",
    "        # Shift z so that it is equivalent to a sample drawn from N(mu, std)\n",
    "        #TODO\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode the data and output the estimated parameters mu and logvar\n",
    "        #TODO\n",
    "        # mu can be defined as the first half of the encoder output\n",
    "        self.mu = #TODO\n",
    "        # logvar can be defined as the second half of the encoder output\n",
    "        self.logvar = #TODO\n",
    "\n",
    "        # Reparameterization: draw a sample z of shape (N, z_dim) (z~N(mu, logvar))\n",
    "        # by calling the reparameterize method\n",
    "        #TODO\n",
    "\n",
    "        # Generate (decode) an image from the sample z\n",
    "        #TODO\n",
    "        return out\n",
    "\n",
    "    def generate(self, N_imgs):\n",
    "        \"\"\"\n",
    "        Generate new images by giving sampled latent vectors from N(0,1) to the decoder\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        return imgs_generated\n",
    "\n",
    "def loss_VAE(inputs, outputs, mu, logvar):\n",
    "    \"\"\"\n",
    "    Loss for a VAE: a reconstruction term (mse loss) and a distribution term (kl divergence)\n",
    "    \"\"\"\n",
    "    # Regular reconstruction term using the mse loss, same as what we used for the AutoEncoder\n",
    "    mse_loss = #TODO\n",
    "    # Distribution term: force the latent space to behave like a normal distribution\n",
    "    # Special case of the KL divergence when the prior is Q~N(0,1) and P~N(mu, std)\n",
    "    kld_loss = #TODO\n",
    "    return mse_loss, kld_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your Variational AutoEncoder\n",
    "\n",
    "Run the cell below to train your VAE. You can play with the parameter if you want/need.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all, we will provide examples of results so that you can still answer the questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "z_dim = 15\n",
    "\n",
    "vae = MyVAE(z_dim=z_dim)\n",
    "vae.to(device=device)\n",
    "\n",
    "train_loader_imgs = torch.utils.data.DataLoader(imgs_train, batch_size=512, shuffle=True)\n",
    "val_loader_imgs = torch.utils.data.DataLoader(imgs_val, batch_size=512, shuffle=True)\n",
    "\n",
    "lr = 0.3\n",
    "momentum = 0.7\n",
    "\n",
    "optimizer = optim.SGD(vae.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = loss_VAE\n",
    "\n",
    "training_vae(\n",
    "    n_epochs = 30,\n",
    "    optimizer = optimizer,\n",
    "    model = vae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_generated_images(vae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training loss\n",
    "\n",
    "![Example of VAE training loss (see VAE_vanilla_training_loss image)](./VAE_vanilla_training_loss.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Compare the evolution of the MSE loss with the KL-divergence loss and comment. \n",
    "2. By looking at the loss what type of results do you expect? (Good reconstruction? Good distribution in the latent space?)\n",
    "3. Recalling that for this assignment (and in its specific settings) we can say that our VAE learns well how to reconstruct if its training loss is lower than 0.6, would you say that the VAE is able to reconstruct well our data? \n",
    "\n",
    "### Example reconstruction\n",
    "![Example of VAE reconstruction results on the training dataset (see VAE_vanilla_train_reconstruction image)](./VAE_vanilla_train_reconstruction.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Comment the behavior of the VAE when reconstructing different images.\n",
    "2. Mode collapse is a very common problem when training VAE (and other unsupervised method), can you explain what this problem is?\n",
    "\n",
    "### Example generation\n",
    "![Example of VAE generation results (see VAE_vanilla_generation image)](./VAE_vanilla_generation.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Do the generated images look similar to the outputs of the reconstructed images?\n",
    "1. Would you say that the VAE learnt well the parameters of a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your Variational AutoEncoder: with a weighted KLD loss\n",
    "\n",
    "In order to fix the 'mode collapse' problem and to make sure our model learns to both reconstruct images and organize the latent space we will add a weight on the KL divergence term. We will first define this weight as extremely low so that the model learns first how to reconstruct and then we will increasingly amplify this weight so that the model re-organize the latent space properly. Once the weight is considered high enough we will stop increasing this weight let the model re-polish its reconstruction learning in this new organization. \n",
    "\n",
    "Run the cell below to train your VAE. You can play with the parameter if you want/need.\n",
    "Keep in mind that training unsupervised model is not easy and that it is okay in this assignment if you don't get good results at all, we will provide examples of results so that you can still answer the questions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "kld_weight = torch.zeros(epochs)\n",
    "kld_weight[0] = 0.0000001\n",
    "for i in range(1,81):\n",
    "     kld_weight[i] = kld_weight[i-1]*1.18\n",
    "kld_weight[81:] = kld_weight[80]\n",
    "plt.plot(np.arange(len(kld_weight)), kld_weight)\n",
    "plt.title(\"KLD Weight term evolution thoughout the epochs \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MyVAE(z_dim=z_dim)\n",
    "vae.to(device=device)\n",
    "optimizer = optim.SGD(vae.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "training_vae(\n",
    "    n_epochs = epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = vae,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader_imgs,\n",
    "    kld_weight = kld_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(device=torch.device('cpu')) \n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_train)\n",
    "fig.suptitle(\"Training dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_true_VS_reconstructed(vae, imgs_val)\n",
    "fig.suptitle(\"Validation dataset\", fontsize=15)\n",
    "plt.show()\n",
    "fig, axs = plot_generated_images(vae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example training loss\n",
    "\n",
    "![Example of VAE training loss with the KLD weight term (see VAE_training_loss image)](./VAE_training_loss.png)\n",
    "\n",
    "![Example of VAE training loss with the KLD weight term (see VAE_training_loss_end image)](./VAE_training_loss_end.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the 2 images above.\n",
    "\n",
    "1. Compare the evolution of the MSE loss with the KL-divergence loss and comment. \n",
    "2. By looking at the loss what type of results do you expect? (Good reconstruction? Good distribution in the latent space?)\n",
    "3. Recalling that for this assignment (and in its specific settings) we can say that our VAE learns well how to reconstruct if its training loss is lower than 0.6, would you say that the VAE is able to reconstruct well our data? \n",
    "\n",
    "### Example reconstruction\n",
    "![Example of VAE reconstruction results on the training dataset with the KLD weight term (see VAE_train_reconstruction image)](./VAE_train_reconstruction.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Comment the behavior of the VAE when reconstructing different images.\n",
    "\n",
    "### Example generation\n",
    "![Example of VAE generation results with the KLD weight term (see VAE_generation image)](./VAE_generation.png)\n",
    "\n",
    "--------------------\n",
    "## TODO\n",
    "Analyse the results in the image above.\n",
    "\n",
    "1. Do the generated images look similar to the outputs of the reconstructed images?\n",
    "1. Would you say that the VAE learnt well the parameters of a normal distribution?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
